# ğŸš€ Fine-Tuning Falcon-RW-1B with QLoRA

This project demonstrates how to **fine-tune a large language model (Falcon-RW-1B)** using **QLoRA** on a **custom instruction dataset**.  
The goal was to build a resource-efficient chatbot capable of answering questions in a conversational style.

---

## ğŸ“Œ Project Overview

- **Base Model** â†’ Falcon-RW-1B 
- **Dataset Used** â†’ Guanaco-LLaMA2-1K (instruction-tuning dataset)
- **Fine-Tuning Method** â†’ **QLoRA** (Quantized LoRA) 
- **Frameworks & Tools** â†’ Hugging Face Transformers, TRL, PEFT, BitsAndBytes, TensorBoard
- **Output** â†’ A fine-tuned chatbot model saved as falcon-1b-finetune


---

## âš¡ Why QLoRA?

- Fine-tuning large models is **memory heavy.**
- LoRA trains only **small adapter layers,** but still requires model in full precision.
- **QLoRA** combines LoRA +**4-bit quantization,** allowing fine-tuning on **smaller GPUs** with much lower VRAM usage.

---

## ğŸ›  Tech Stack

- **Hugging Face Transformers** â†’ Load & manage LLMs.
- **TRL (Transformers Reinforcement Learning)** â†’ Provides SFTTrainer for supervised fine-tuning.
- **PEFT (Parameter-Efficient Fine-Tuning)** â†’ Implements LoRA adapters. 
- **BitsAndBytes (bnb)** â†’ Enables 4-bit quantization for QLoRA.
- **TensorBoard** â†’ Training visualization (loss curves, logs).
- **PyTorch** â†’ Core deep learning framework.

---

## ğŸ”§ Training Configuration

### LoRA Parameters
- **lora_r = 64** â†’ Rank of LoRA matrices
- **lora_alpha = 16** â†’ Scaling factor
- **lora_dropout = 0.1** â†’ Prevents overfitting

### ğŸ”¹ Step 2: Semantic Chunking
- Uses **SemanticChunker** with **Google Embeddings**  
- Breaks the document into **meaningful sections** instead of fixed-size chunks

### ğŸ”¹ Step 3: Vector Database (ChromaDB)
- Stores embeddings in **ChromaDB (persistent)**  
- Automatically reuses existing DB if available

### ğŸ”¹ Step 4: Query Retrieval
- User query â†’ Expanded into multiple queries using **MultiQueryRetriever**  
- Fetches **top k=3** most relevant chunks from Chroma

### ğŸ”¹ Step 5: Prompt + LLM
- Builds a **prompt template** with:  
  - Conversation history  
  - Retrieved context  
  - Latest question  
- Sends it to **Google Gemini (`gemini-1.5-flash`)**

### ğŸ”¹ Step 6: Streaming Response
- LLM response streamed **word-by-word**  
- Typing effect for a **natural chat experience**


---
s
## ğŸ“Š Example Workflow

### 1. Start the App
- Loads **PDF** + **Chroma DB**

### 2. Ask a Question
- Example: *â€œSummarize key pointsâ€*

### 3. Retrieve Relevant Chunks
- Retriever fetches **top matching sections** from the document

### 4. Generate Response
- **Gemini LLM** creates a contextual answer

### 5. Display Answer
- Chat interface shows the response with a **typing animation**

---

## ğŸ“· Screenshots  

### 1. Welcome Page  
![Welcome Page](screenshots/welcome.png)  

### 2. User Asking Question  
![User Asking Question](screenshots/user_question.png)  

### 3. Bot Responding with Typing Effect  
![Bot Response](screenshots/bot_response.png)  


---

## ğŸ“Œ Dependencies

### ğŸ Python
- Python **3.9+**

### ğŸ“¦ Core Libraries
- **Streamlit**  
- **LangChain**  
- **LangChain-Community**  
- **LangChain-Chroma**

### ğŸ—„ï¸ Optional
- **FAISS** â†’ optional backup vector database

### ğŸ¤– AI Model
- **Google Generative AI (Gemini)**

3. **Install via:**
   ```bash
   pip install -r requirements.txt

---

