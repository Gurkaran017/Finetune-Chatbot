# ðŸš€ Fine-Tuning Falcon-RW-1B with QLoRA

This project demonstrates how to **fine-tune a large language model (Falcon-RW-1B)** using **QLoRA** on a **custom instruction dataset**.  
The goal was to build a resource-efficient chatbot capable of answering questions in a conversational style.

---

## ðŸ“Œ Project Overview

- **Base Model** â†’ Falcon-RW-1B 
- **Dataset Used** â†’ Guanaco-LLaMA2-1K (instruction-tuning dataset)
- **Fine-Tuning Method** â†’ **QLoRA** (Quantized LoRA) 
- **Frameworks & Tools** â†’ Hugging Face Transformers, TRL, PEFT, BitsAndBytes, TensorBoard
- **Output** â†’ A fine-tuned chatbot model saved as falcon-1b-finetune


---

## âš¡ Why QLoRA?

- Fine-tuning large models is **memory heavy.**
- LoRA trains only **small adapter layers,** but still requires model in full precision.
- **QLoRA** combines LoRA +**4-bit quantization,** allowing fine-tuning on **smaller GPUs** with much lower VRAM usage.

---

## ðŸ›  Tech Stack

- **Hugging Face Transformers** â†’ Load & manage LLMs.
- **TRL (Transformers Reinforcement Learning)** â†’ Provides SFTTrainer for supervised fine-tuning.
- **PEFT (Parameter-Efficient Fine-Tuning)** â†’ Implements LoRA adapters. 
- **BitsAndBytes (bnb)** â†’ Enables 4-bit quantization for QLoRA.
- **TensorBoard** â†’ Training visualization (loss curves, logs).
- **PyTorch** â†’ Core deep learning framework.

---

## ðŸ”§ Training Configuration

### LoRA Parameters
- **lora_r = 64** â†’ Rank of LoRA matrices
- **lora_alpha = 16** â†’ Scaling factor
- **lora_dropout = 0.1** â†’ Prevents overfitting

### Quantization Parameters (QLoRA)
- **use_4bit = True** â†’ Enable 4-bit quantization
- **bnb_4bit_compute_dtype = "float16"** â†’ Math done in half precision
- **bnb_4bit_quant_type = "nf4"** â†’ NormalFloat4 (better accuracy)
- **use_nested_quant = False** â†’ No double quantization

### Training Arguments
- **epochs = 1** â†’ Trained for 1 pass over dataset
- **batch_size = 4** â†’ Training batch size
- **gradient_checkpointing = True** â†’ Saves GPU memory
- **learning_rate = 2e-4** â†’ Optimizer learning rate
- **lr_scheduler = cosine** â†’ Learning rate decay strategy
- **logging_steps = 25** â†’ Log every 25 steps
- **report_to = "tensorboard"** â†’ Logs to TensorBoard

---

## ðŸ“‰ Training Visualization (TensorBoard)

- During training, TensorBoard was used to track progress:

- **Training Loss Curve** â†’ Shows model improving over steps.
- **Logs** â†’ Training speed, memory usage, optimizer info.

- **Command to launch TensorBoard:**
   ```bash
   %load_ext tensorboard
   %tensorboard --logdir results/runs

---

## ðŸ“· Screenshots  

![Welcome Page](screenshots/tensorboard.png)  

![User Asking Question](screenshots/tensorboard2.png)  

---
## ðŸ“Š Training Workflow

- **Load Dataset** â†’ Instruction dataset (guanaco-llama2-1k) with question-answer pairs.
- **Load Base Model** â†’ Falcon-RW-1B loaded in 4-bit precision.
- **Attach LoRA Adapters** â†’ Small trainable layers added.
- **Fine-Tune with TRLâ€™s SFTTrainer** â†’ Only LoRA parameters trained, base model stays frozen.
- **Monitor Training** â†’ Loss visualized in TensorBoard.
- **Save Fine-Tuned Model** â†’ Exported to falcon-1b-finetune.

---

